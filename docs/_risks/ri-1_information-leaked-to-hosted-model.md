---
sequence: 1
title: Information Leaked To Hosted Model
layout: risk
doc-status: Draft
type: RC
owasp-llm_references:
  - llm06-2025  # LLM06:2025 Excessive Agency
nist-ai-600-1_references:
  - 2-4  # 2.4. Data Privacy
  - 2-9  # 2.9. Information Security
ffiec-itbooklets_references:
  - sec-2  # SEC: II Information Security Program Management
  - sec-3  # SEC: III Security Operations
  - ots-2  # OTS: Risk Management
eu-ai-act_references:
  - c3-s2-a10  # III.S2.A10: Data and Data Governance
  - c3-s2-a13  # III.S2.A13: Transparency and Provision of Information to Deployers
  - c5-s2-a53  # V.S2.A53: Obligations for Providers of General-Purpose AI Models
---
## Summary

The transmission of sensitive data to third-party hosted Large Language Model (LLM) platforms for inference or fine-tuning introduces a significant risk of information leakage for financial institutions. This risk encompasses the potential exposure of confidential organizational data, including customer Personally Identifiable Information (PII), Non-Public Market Information (NPMI), proprietary trading algorithms, internal risk assessments, and strategic plans. Such leakage can occur due to inadequate data governance, insufficient control measures within the hosted environment, or inherent characteristics of LLM technology.

## Description

A core challenge arises from the nature of interactions with external LLMs, which can be conceptualized as a **two-way trust boundary**. Neither the data inputted into the LLM nor the output received can be fully trusted by default. Inputs containing sensitive financial information may be retained or processed insecurely by the provider, while outputs may inadvertently reveal previously processed sensitive data, even if the immediate input prompt appears benign.

Several mechanisms unique to or amplified by LLMs contribute to this risk:

* **Model Memorization and Overfitting:** LLMs, particularly very large ones, can inadvertently [memorize](https://arxiv.org/pdf/2310.18362) and retain portions of the data they have processed during training, fine-tuning, or even through user interactions. [Overfitting](https://aws.amazon.com/what-is/overfitting/) during the training process can exacerbate this. Consequently, sensitive information—such as specific customer details from a CRM excerpt, conditions from a confidential loan agreement, or elements of a proprietary financial strategy fed into the model—might be recalled and disclosed in subsequent, unrelated sessions, potentially even to different users (cross-user leakage).

* **Prompt-Based Attacks:** As detailed in the "Prompt Injection" risk (ri-10), adversarial actors can craft malicious prompts to manipulate the LLM into revealing sensitive information it has access to or has previously memorized. This method directly targets the model's ability to recall and synthesize information.

* **Inadequate Data Sanitization and Filtering:** The risk of inadvertent disclosure significantly increases if the LLM service provider or the financial institution itself fails to implement robust data sanitization, anonymization, or filtering mechanisms for both data inputted into the model and outputs generated by it.

The risk profile can be further influenced by the provider's data handling practices and the specific services utilized:

* **Data Retention, Logging, and Usage Policies:** The terms of use, data processing addendums, and actual data handling practices of the LLM provider are critical. Without clear contractual agreements and technical assurances regarding data encryption, retention limits, purpose limitation for data usage (e.g., preventing use for future model training without consent), and secure deletion, financial institutions lose visibility and control over their sensitive data. Hosted models may not always provide transparent mechanisms for how input data is processed or retained, increasing the risk of persistent exposure.

* **Fine-Tuning on Proprietary Data:** Fine-tuning an LLM on a financial institution's proprietary datasets (e.g., internal reports, customer communication logs, specialized financial documents) can embed sensitive details deeply within the model. If this fine-tuning is performed by the third-party provider, or if access controls to the fine-tuned model are not stringently managed, this sensitive embedded information may become accessible to unauthorized individuals or through model queries, potentially violating principles of least privilege.

It is important to conduct thorough due diligence, as the level of data protection can vary significantly between LLM offerings. Enterprise-grade commercial LLMs may offer features like private endpoints, options to disable data retention for model training, data encryption in transit and at rest, and stronger contractual safeguards regarding data ownership and usage. Conversely, free or consumer-grade services might have less stringent data protection measures and may explicitly state that user data will be used for future model improvements. Regardless of the provider's tier, ongoing vigilance and adherence to internal data governance policies are paramount.

### Consequences

The consequences of such information leakage for a financial institution can be severe:
* **Breach of Data Privacy Regulations:** Unauthorized disclosure of PII can lead to significant fines under regulations like GDPR, CCPA, and others, alongside mandated customer notifications.
* **Violation of Financial Regulations:** Leakage of confidential customer information or market-sensitive data can breach specific financial industry regulations concerning data security and confidentiality (e.g., GLBA in the US).
* **Loss of Competitive Advantage:** Exposure of proprietary algorithms, trading strategies, or confidential business plans can erode a firm's competitive edge.
* **Reputational Damage:** Public disclosure of sensitive data leakage incidents can lead to a substantial loss of customer trust and damage to the institution's brand.
* **Legal Liabilities:** Beyond regulatory fines, institutions may face lawsuits from affected customers or partners.


### Key Risks 

- **Two-Way Trust Boundary**: The client-to-LLM interaction introduces a two-way trust boundary where neither input nor output can be fully trusted. This makes it critical to assume the output could leak sensitive information unintentionally, even when the input appears benign.
- **Model Overfitting and Memorization**: LLMs may retain sensitive data introduced during training, leading to unintentional data leakage in future interactions. This includes potential cross-user leakage, where one user's sensitive data might be disclosed to another.
- **External Inference Endpoint Risks**: Hosted models may not provide transparent mechanisms for how input data is processed, retained, or sanitized, increasing the risk of persistent exposure of proprietary data.

This risk is aligned with OWASP’s [LLM06: Sensitive Information Disclosure](https://genai.owasp.org/llmrisk/llm06-sensitive-information-disclosure/), which highlights the dangers of exposing proprietary or personally identifiable information (PII) through large-scale, externally hosted AI systems.

## Links

- [FFIEC IT Handbook](https://ithandbook.ffiec.gov/)
- [Scalable Extraction of Training Data from (Production) Language Models](https://arxiv.org/abs/2311.17035)
